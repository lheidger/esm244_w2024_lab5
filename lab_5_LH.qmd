---
title: "Lab 5"
author: "Lily Heidger"
format: html
editor: visual
date: "2/4/2024"
format: 
  html:
    code-fold: show
    embed-resources: true
    toc: true
execute:
  message: false
  warning: false
---

# Summary: 
We want to create a mmodel we can use in the field to quickly and easily estimate a penguin's mass based on the subset of data in the 'palmerpenguins' package. 

Objectives: 

* Set up several competing models
  * data cleaning and prep
* Compare their fit using information criteria
* Compare models using cross-validation
  * Practice writing functions
  * Iterating with for loops and purrr
  
# Set up some models

## Clean our data

```{r}
library(tidyverse)
library(palmerpenguins)
library(AICcmodavg)
library(equatiomatic)
```


```{r}
penguins_clean <- penguins |> drop_na() |>
  rename(mass = body_mass_g, 
         bill_l = bill_length_mm, 
         bill_d = bill_depth_mm, 
         flip_l = flipper_length_mm)

summary(penguins_clean)
```

## Create a linear regression model

```{r}
mdl1 <- lm(formula = mass ~ bill_l + bill_d + flip_l + species + sex + island, data = penguins_clean)
```

```{r}
f1 <- mass ~ bill_l + bill_d + flip_l + species + sex + island
mdl1 <- lm(formula = f1, data = penguins_clean)

f2 <- mass ~ bill_l + bill_d + flip_l + species + sex
mdl2 <- lm(f2, data = penguins_clean)
summary(mdl2)

f3 <- mass ~ bill_d + flip_l + species + sex
mdl3 <- lm(f3, data= penguins_clean)
summary(mdl3)
```

## Compare models using AIC and BIC

```{r}
AIC(mdl1, mdl2, mdl3)
#model 2 has lowest score, so best AIC

BIC(mdl1, mdl2, mdl3)

AICcmodavg::aictab(list(mdl1, mdl2, mdl3))
bictab(list(mdl1, mdl2, mdl3))
```

## Compare using cross-validation

We have a dataset of 333 penguin observations.
We have 3 models we'd like to compare. 
We want to use cross validation - 10-fold cross validation-  focus on model 1.

### Psuedocode

Finding RMSE for each model
Build a function to create a list of models and iterate over each of the models
Split the data into 10 pieces - one for test, and do some iterations
At the end, compare the success rate of each of the models
Reserve the 1/10th as the test, the other 9/10ths is training
Randomly choose the data that we reserve
For each of the 10 folds, make a model, make RMSE of each based on the 1/10th reserve, and then do that 10 times. 


```{r}
folds <- 10
fold_vec <- rep(1:folds, length.out = nrow(penguins_clean)) #creates a vector that repeats 1:10 alongside the entire length of the penguins dataset

set.seed(42) # random sampling

penguins_fold <- penguins_clean |>
  mutate(group = sample(fold_vec, size = n(), replace = FALSE))
table(penguins_fold$group) #creates evenly sized groups

### Reserve the first fold for testing, and the rest for training
test_df <- penguins_fold |>
  filter(group==1)

train_df <- penguins_fold |>
  filter(group != 1)

```

### Create an RMSE function

RMSE: Root-mean-square error
```{r}
calc_rmse <- function(x, y) {
  ### x is a predicted value, y is observed
  rmse <- (x-y)^2 |> mean() |> sqrt()
  return(rmse)
}
```

### Train the model on training set

```{r}
training_lm1 <- lm(f1, data = train_df)
summary(training_lm1)
training_lm2 <- lm(f2, data = train_df)
training_lm3 <- lm(f3, data = train_df)
```

### Compare models using RMSE based on first fold

```{r}
predict_test <- test_df %>%
  mutate(model1 = predict(training_lm1, test_df),
         model2 = predict(training_lm2, .), 
         model3 = predict(training_lm3, .))
rmse_predict_test <- predict_test %>%
  summarize(rmse_mdl1 = calc_rmse(model1, mass),
            rmse_mdl2 = calc_rmse(model2, mass),
            rmse_mdl2 = calc_rmse(model3, mass)) 
```

### 10-fold cross validation using a 'for' loop

for loop cycles over each element in a sequence (vector or list) and performs some set of operations using that element
```{r}
# month.name
# for(m in month.name) {
#   print(paste('Month: ', m))
# }
```


```{r}
### initialize an empty vector
rmse_vec <- vector(length = folds)

for(i in 1:folds) {
  ### split into training and testing
  kfold_test_df <- penguins_fold %>%
    filter(group == i)
  kfold_train_df <- penguins_fold %>%
    filter(group != i)
  ### train
  kfold_lm1 <- lm(f1, data = kfold_train_df)
  ### test against test set
  kfold_pred_df <- kfold_test_df %>%
    mutate(mdl = predict(kfold_lm1, .))
  kfold_rmse <- kfold_pred_df %>%
    summarize(rmse_mdl = calc_rmse(mdl, mass))
  ### save output
  rmse_vec[i] <- kfold_rmse$rmse_mdl 
}

mean(rmse_vec)
```
```{r}
kfold_cv <- function(i, df, formula) {
  ### split into training and testing
  kfold_test_df <- df %>%
    filter(group == i)
  kfold_train_df <- df %>%
    filter(group != i)
  
  ### train model
  kfold_lm <- lm(formula, data = kfold_train_df)
  
  ### test against test set
  kfold_pred_df <- kfold_test_df %>%
    mutate(mdl = predict(kfold_lm, .))
  kfold_rmse <- kfold_pred_df %>%
    summarize(rmse_mdl = calc_rmse(mdl, mass))
  ### save output
  return(kfold_rmse$rmse_mdl) 
  
}
```

```{r}
#kfold_cv(i = 1, df = penguins_fold, formula = f1)
rmse_loop_vec <- vector(length = folds)
for(i in 1:folds) {
  rmse_loop_vec[i] <- kfold_cv(i = i, df = penguins_fold, formula = f1)
}
mean(rmse_loop_vec)
```
### Cross validation using 'purrr::map()'

Map a function to a sequence of inputs (vector or list)

```{r}
map(month.name, nchar)
map_int(month.name, nchar)
```
```{r}
rmse_map_list <- purrr::map(.x = 1:10, 
                            .f = kfold_cv,
                            df = penguins_fold,
                            formula = f1)
rmse_map_list %>% unlist() %>% mean()
```

```{r}
rmse_df <- data.frame(j = 1:folds) %>%
  mutate(rmse_mdl1 = map_dbl(.x = j, .f = kfold_cv, df = penguins_fold, formula = f1),
         rmse_mdl2 = map_dbl(.x = j, .f = kfold_cv, df = penguins_fold, formula = f2),
         rmse_mdl3 = map_dbl(.x = j, .f = kfold_cv, df = penguins_fold, formula = f3))

rmse_means <- rmse_df %>%
  summarize(across(starts_with('rmse'), mean))
```


```{r}
final_mdl <- lm(f2, data = penguins_clean)
```



